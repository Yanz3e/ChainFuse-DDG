# -*- coding: utf-8 -*-
import argparse
import os
from pathlib import Path
import re
import csv
import math
import json
import torch
import joblib
import numpy as np
import matplotlib.pyplot as plt

# --------- å·¥å…·å‡½æ•°ï¼šè¯»å†™ ---------
def read_single_fasta(path):
    seq = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            if line.startswith(">"):
                continue
            seq.append(line)
    seq = "".join(seq).strip().upper()
    if not seq:
        raise ValueError(f"Empty FASTA: {path}")
    return seq

def ensure_dir(p):
    p = Path(p)
    p.mkdir(parents=True, exist_ok=True)
    return p

# --------- ä½ç‚¹ / çªå˜æšä¸¾ ---------
AA20 = "ACDEFGHIKLMNPQRSTVWY"

def parse_positions(pos_str, length):
    """
    è§£æç±»ä¼¼ '22-148' æˆ– '22,23,30-35' è¿™æ ·çš„è¾“å…¥ï¼Œè¿”å› 1-based çš„æ•´æ•°åˆ—è¡¨ã€‚
    """
    pos = []
    for tok in re.split(r"[,\s]+", pos_str.strip()):
        if not tok:
            continue
        if "-" in tok:
            a, b = tok.split("-")
            a, b = int(a), int(b)
            if a > b:
                a, b = b, a
            pos.extend(list(range(a, b + 1)))
        else:
            pos.append(int(tok))
    # é™åˆ¶åœ¨åºåˆ—é•¿åº¦å†…
    pos = [p for p in pos if 1 <= p <= length]
    if not pos:
        raise ValueError("positions è§£æä¸ºç©ºï¼Œè¯·æ£€æŸ¥èŒƒå›´æ˜¯å¦è¶…å‡ºåºåˆ—é•¿åº¦")
    return sorted(sorted(set(pos)))

def make_all_mutants(seq, positions, alts="ALL"):
    """
    è¿”å› [(pos, wt, mut, mut_seq), ...]ï¼›pos ä¸º 1-basedã€‚
    """
    if alts == "ALL":
        alphabet = AA20
    else:
        alphabet = "".join([c for c in alts.upper() if c in AA20])

    seq = seq.upper()
    L = len(seq)
    out = []
    for pos in positions:
        wt = seq[pos - 1]
        for a in alphabet:
            if a == wt:
                continue
            ms = list(seq)
            ms[pos - 1] = a
            out.append((pos, wt, a, "".join(ms)))
    return out

def write_fasta(path, records):
    """
    records: [(name, seq)]
    """
    with open(path, "w", encoding="utf-8") as f:
        for name, seq in records:
            f.write(f">{name}\n")
            # wrap å¯é€‰
            for i in range(0, len(seq), 80):
                f.write(seq[i:i+80] + "\n")

# --------- ESM åµŒå…¥ ---------
def load_esm1b_from_ckpt(ckpt_path):
    """
    æœ¬åœ°åŠ è½½ ESM-1b æƒé‡ï¼ˆ.ptï¼‰ï¼Œè¿”å› (model, alphabet)ã€‚
    å…¼å®¹ä¸åŒ fair-esm ç‰ˆæœ¬ï¼šæœ‰çš„æ”¯æŒ regression_locationï¼Œæœ‰çš„ä¸æ”¯æŒã€‚
    """
    import esm
    fn = esm.pretrained.load_model_and_alphabet_local
    try:
        # æ–°ç‰ˆ fair-esm æ”¯æŒ regression_location
        model, alphabet = fn(str(ckpt_path), regression_location=None)
    except TypeError:
        # æ—§ç‰ˆä¸æ”¯æŒè¯¥å‚æ•°ï¼Œç›´æ¥è°ƒç”¨
        model, alphabet = fn(str(ckpt_path))
    model.eval()
    return model, alphabet

def esm_embed_mean(model, alphabet, seqs, device="cuda", batch_size=8, layer=33):
    """
    å¯¹æ¯æ¡åºåˆ—å–è¯¥å±‚è¡¨ç¤ºçš„å‡å€¼ï¼ˆå»æ‰ BOS/EOSï¼‰ã€‚
    è¿”å› numpy array (N, 1280)
    """
    model = model.to(device)
    batch_converter = alphabet.get_batch_converter()

    all_vecs = []
    with torch.no_grad():
        for i in range(0, len(seqs), batch_size):
            chunk = seqs[i:i+batch_size]
            data = [("seq", s) for s in chunk]
            _, _, toks = batch_converter(data)
            toks = toks.to(device)
            out = model(toks, repr_layers=[layer], return_contacts=False)
            reps = out["representations"][layer]  # (B, L, D)
            # å¹³å‡æ—¶å»æ‰ <cls>/<eos>
            # alphabet.toks_to_ids['<cls>']==0, '<eos>'==2ï¼ˆä¸€èˆ¬å¦‚æ­¤ï¼‰
            # ç¨³å¦¥åšæ³•ï¼šæŒ‰æ¯æ¡çœŸå®é•¿åº¦æ©ç 
            for b, s in enumerate(chunk):
                # tokens: [cls] + s + [eos]
                v = reps[b, 1:1+len(s), :].mean(0).detach().cpu().numpy()
                all_vecs.append(v)
    return np.stack(all_vecs, axis=0)

# --------- èƒ½é‡æ¨¡å‹åŠ è½½ï¼šå¥å£®è§£åµŒå¥—ï¼Œè‡ªåŠ¨åŒ¹é… ---------
def _unwrap_nested_state(d):
    if not isinstance(d, dict):
        return d
    for _ in range(6):
        for k in ["state_dict", "state", "model_state", "module", "model", "net", "energy", "params", "weights"]:
            if isinstance(d, dict) and k in d and isinstance(d[k], dict):
                d = d[k]
                break
        else:
            break
    return d

def _walk_tensors(obj, prefix=""):
    items = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            items += _walk_tensors(v, prefix + k + ".")
    else:
        if torch.is_tensor(obj) or hasattr(obj, "shape"):
            items.append((prefix[:-1], obj))
    return items

def load_energy_from_any_ckpt(ckpt_path, Dh, Dl):
    """
    ä»ä»»æ„å½¢æ€ ckpt æå– wH/wL/UH/UL/bï¼ˆè‹¥ä¸å­˜åœ¨æŸé¡¹åˆ™ä¸º None/0ï¼‰ã€‚
    """
    raw = torch.load(ckpt_path, map_location="cpu")
    sd  = _unwrap_nested_state(raw)
    if not isinstance(sd, dict):
        raise RuntimeError("checkpoint ä¸æ˜¯ dict/state_dict ç»“æ„ï¼Œæ— æ³•è§£æ")

    tensors = _walk_tensors(sd)

    def to_np(x):
        if torch.is_tensor(x):
            x = x.detach().cpu().numpy()
        return np.asarray(x)

    # 1) wH/wL: æ‹‰å¹³åé•¿åº¦ == Dh / Dl
    cand_wH = [(k, to_np(v).reshape(-1)) for k, v in tensors if hasattr(v, "shape") and np.prod(v.shape)==Dh]
    cand_wL = [(k, to_np(v).reshape(-1)) for k, v in tensors if hasattr(v, "shape") and np.prod(v.shape)==Dl]

    def pick_best(cands, regex=None):
        if not cands: return None
        if regex:
            for k,v in cands:
                if re.search(regex, k, re.I):
                    return v
        return cands[0][1]

    wH = pick_best(cand_wH, r"(^|\.)(w_h|wh|heavy|h)(\.|$)")
    wL = pick_best(cand_wL, r"(^|\.)(w_l|wl|light|l)(\.|$)")

    # 2) UH/UL: (Dh, r)/(Dl, r)
    cand_UH = [(k, to_np(v)) for k,v in tensors if hasattr(v,"shape") and len(v.shape)==2 and v.shape[0]==Dh]
    cand_UL = [(k, to_np(v)) for k,v in tensors if hasattr(v,"shape") and len(v.shape)==2 and v.shape[0]==Dl]
    UH = UL = None
    if cand_UH and cand_UL:
        by_rank = {}
        for k,v in cand_UH:
            by_rank.setdefault(v.shape[1], []).append(v)
        for k,v in cand_UL:
            r = v.shape[1]
            if r in by_rank:
                UH, UL = by_rank[r][0], v
                break
        if UH is None:
            UH = cand_UH[0][1]
        if UL is None:
            UL = cand_UL[0][1]

    # 3) biasï¼šå–ç¬¬ä¸€ä¸ªæ ‡é‡
    b = 0.0
    for k, v in tensors:
        arr = to_np(v)
        if arr.size == 1:
            b = float(arr.reshape(-1)[0]); break

    print("[ckpt] parsed:",
          f"wH={None if wH is None else wH.shape}",
          f"wL={None if wL is None else wL.shape}",
          f"UH={None if UH is None else UH.shape}",
          f"UL={None if UL is None else UL.shape}",
          f"b={b}")
    if (wH is None) and (wL is None) and (UH is None) and (UL is None):
        raise RuntimeError("æœªåœ¨ ckpt ä¸­åŒ¹é…åˆ°ä»»ä½•å¯ç”¨æƒé‡ï¼ˆè¯·æ£€æŸ¥ Dh/Dl ä¸ ckpt æ˜¯å¦ä¸€è‡´ï¼‰")
    return {"wH": wH, "wL": wL, "UH": UH, "UL": UL, "b": b}

# --------- èƒ½é‡/Î”Î”G è®¡ç®— ---------
def standardize(x, scaler):
    # x: (N,D)
    return scaler.transform(x)

def energy_from_parts(zH, zL, scalerH, scalerL, wH=None, wL=None, UH=None, UL=None, b=0.0, device="cpu"):
    """
    zH/zL: numpy (N, D)
    è¿”å› numpy (N,)
    """
    e = np.zeros((zH.shape[0],), dtype=np.float32) + float(b)

    if scalerH is not None and zH is not None:
        zHn = standardize(zH, scalerH)  # (N,Dh)
    else:
        zHn = None
    if scalerL is not None and zL is not None:
        zLn = standardize(zL, scalerL)  # (N,Dl)
    else:
        zLn = None

    if (wH is not None) and (zHn is not None):
        e += (zHn @ wH.astype(np.float32))
    if (wL is not None) and (zLn is not None):
        e += (zLn @ wL.astype(np.float32))

    if (UH is not None) and (zHn is not None):
        t = zHn @ UH.astype(np.float32)           # (N,r)
        e += np.sum(t*t, axis=1)
    if (UL is not None) and (zLn is not None):
        t = zLn @ UL.astype(np.float32)
        e += np.sum(t*t, axis=1)

    return e

def ddg_from_energy(zH_wt, zL_wt, zH_mut, zL_mut, scalerH, scalerL, parts, device="cpu"):
    wH, wL, UH, UL, b = parts["wH"], parts["wL"], parts["UH"], parts["UL"], parts["b"]
    e_wt  = energy_from_parts(zH_wt, zL_wt, scalerH, scalerL, wH, wL, UH, UL, b, device=device)  # (1,)
    e_mut = energy_from_parts(zH_mut, zL_mut, scalerH, scalerL, wH, wL, UH, UL, b, device=device) # (N,)
    return e_mut - e_wt[0]

# --------- æ ¡å‡†ï¼ˆå¯é€‰ï¼‰ ---------
def safe_load_calibrator(path):
    """
    é¿å… joblib ååºåˆ—åŒ–æ—¶å› ç±»è·¯å¾„ä¸åœ¨å½“å‰ __main__ æŠ¥é”™ã€‚
    åªè¦æ±‚å¯¹è±¡æœ‰ predict(X) æ¥å£ã€‚
    """
    obj = joblib.load(path)
    # å…œåº•ï¼šnp.ndarray/å¸¸é‡ å°±å½“æ’ç­‰
    if hasattr(obj, "predict"):
        return obj
    # å¦‚æœæ˜¯ sklearn çš„ IsotonicRegressionï¼Œä¹Ÿæœ‰ predict
    return None

# --------- ä¸»æµç¨‹ ---------
def main():
    ap = argparse.ArgumentParser("Predict Î”Î”G for single mutants with ESM-1b + energy model")
    ap.add_argument("--wt", required=True, help="WT FASTA è·¯å¾„")
    ap.add_argument("--positions", required=True, help="çªå˜ä½ç‚¹ï¼Œå¦‚ '22-148' æˆ– '22,35,50-60'")
    ap.add_argument("--alts", default="ALL", help="å¤‡é€‰æ°¨åŸºé…¸é›†åˆï¼Œé»˜è®¤ ALL=20AA")
    ap.add_argument("--esm-ckpt", required=True, help="ESM-1b æœ¬åœ°æƒé‡ .pt")
    ap.add_argument("--device", default="cuda")
    ap.add_argument("--batch-size", type=int, default=8)

    ap.add_argument("--model", required=True, help="èƒ½é‡æ¨¡å‹ ckptï¼ˆä»»æ„åµŒå¥—/æ‰å¹³æ ¼å¼éƒ½å¯ï¼‰")
    ap.add_argument("--scaler-h", required=True, help="StandardScaler H pkl")
    ap.add_argument("--scaler-l", required=True, help="StandardScaler L pkl")
    ap.add_argument("--calib", default=None, help="å¯é€‰ï¼šæ ¡å‡†å™¨ pklï¼ˆä¸ä¼ åˆ™ä¸æ ¡å‡†ï¼‰")

    ap.add_argument("--mut-dir", default="Sample_Mutant", help="å˜ä½“ä¸åµŒå…¥ä¿å­˜æ ¹ç›®å½•")
    ap.add_argument("--out-dir", default="output", help="ç»“æœè¾“å‡ºç›®å½•")
    ap.add_argument("--tag", default="VHH", help="æ ·æœ¬ tagï¼Œç”¨äºå½’æ¡£")

    ap.add_argument("--topk", type=int, default=10, help="å¯¼å‡º Top-K æ¡ç›®")
    ap.add_argument("--top-mode", choices=["neg","pos","abs"], default="neg",
                    help="Top-K çš„æ’åºæ¨¡å¼ï¼šæœ€è´Ÿ/æœ€æ­£/æŒ‰ç»å¯¹å€¼")

    args = ap.parse_args()

    device = args.device if (args.device == "cpu" or torch.cuda.is_available()) else "cpu"

    # 1) è¯» WT å¹¶æšä¸¾æ‰€æœ‰çªå˜
    wt = read_single_fasta(args.wt)
    positions = parse_positions(args.positions, len(wt))
    muts = make_all_mutants(wt, positions, args.alts)

    mut_root = ensure_dir(Path(args.mut_dir) / args.tag)
    out_root = ensure_dir(args.out_dir)
    emb_root = ensure_dir(mut_root / "emb")

    fasta_all = mut_root / "ALL_mutants.fasta"
    write_fasta(fasta_all, [(f"{p}_{w}>{a}", s) for (p,w,a,s) in muts])
    print(f"ğŸ§¬ æ‰€æœ‰çªå˜åºåˆ—ä¿å­˜ï¼š{fasta_all}")

    # 2) ESM åµŒå…¥
    print(f"ğŸ§  ä½¿ç”¨æœ¬åœ° ESM-1b æƒé‡ï¼š{args.esm_ckpt}")
    model, alphabet = load_esm1b_from_ckpt(Path(args.esm_ckpt))
    model = model.eval()

    wt_vec = esm_embed_mean(model, alphabet, [wt], device=device, batch_size=1)  # (1,1280)
    mut_vecs = esm_embed_mean(model, alphabet, [s for (_,_,_,s) in muts], device=device, batch_size=args.batch_size)

    # ä¿å­˜åµŒå…¥
    np.save(emb_root / "WT.npy", wt_vec.astype(np.float32))
    np.save(emb_root / "ALL_mutants.npy", mut_vecs.astype(np.float32))

    # 3) åŠ è½½ scalerï¼Œç¡®å®š Dh/Dl
    scH = joblib.load(args.scaler_h)
    scL = joblib.load(args.scaler_l)
    Dh = int(getattr(scH, "n_features_in_", None) or len(scH.mean_))
    Dl = int(getattr(scL, "n_features_in_", None) or len(scL.mean_))

    if wt_vec.shape[1] != Dh:
        # å¸¸è§ï¼šESM-1b æ˜¯ 1280 ç»´ï¼›Scaler ä¹Ÿæ˜¯ 1280 ç»´
        print(f"[warn] WT åµŒå…¥ç»´åº¦ {wt_vec.shape[1]} ä¸ scaler_H ç»´åº¦ {Dh} ä¸ä¸€è‡´ã€‚è‹¥ç¡®å®ä¸åŒ¹é…ä¼šæŠ¥é”™ã€‚")
    if mut_vecs.shape[1] != Dh:
        print(f"[warn] Mut åµŒå…¥ç»´åº¦ {mut_vecs.shape[1]} ä¸ scaler_H ç»´åº¦ {Dh} ä¸ä¸€è‡´ã€‚")

    # VHHï¼šæ²¡æœ‰ L é“¾ï¼ŒzL ç½®ä¸º None æˆ– 0 å‘é‡éƒ½å¯ä»¥ï¼›
    # è¿™é‡Œç»Ÿä¸€èµ°æ¥å£ï¼Œè‹¥æ¨¡å‹é‡Œæ²¡æœ‰ wL/UL ä¼šè¢«è‡ªåŠ¨è·³è¿‡ã€‚
    zH_wt  = wt_vec
    zH_mut = mut_vecs
    zL_wt  = None
    zL_mut = None

    # 4) åŠ è½½èƒ½é‡æ¨¡å‹ï¼ˆå¥å£®è§£æï¼‰
    parts = load_energy_from_any_ckpt(args.model, Dh, Dl)
    # è½¬ numpy float32
    for k in ["wH","wL","UH","UL"]:
        if parts[k] is not None:
            parts[k] = parts[k].astype(np.float32)

    # 5) é¢„æµ‹ Î”Î”G
    ddg = ddg_from_energy(zH_wt, zL_wt, zH_mut, zL_mut, scH, scL, parts, device=device)  # (N,)

    # 6) å¯é€‰æ ¡å‡†
    ddg_cal = None
    if args.calib:
        try:
            cal_obj = safe_load_calibrator(args.calib)
            if cal_obj is not None:
                ddg_cal = cal_obj.predict(ddg.reshape(-1,1)).astype(np.float32)
                print(f"ğŸ“ å·²åº”ç”¨æ ¡å‡†å™¨ï¼š{args.calib}")
            else:
                print(f"[warn] æ ¡å‡†å™¨ {args.calib} ä¸å¯ç”¨ï¼ˆæ—  predictï¼‰ï¼Œå°†è·³è¿‡ã€‚")
        except Exception as e:
            print(f"[warn] æ ¡å‡†å™¨åŠ è½½å¤±è´¥ï¼š{e}ï¼Œå°†è·³è¿‡ã€‚")

    # 7) å†™å‡º CSV
    out_csv = out_root / f"{args.tag}_ALL_ddg.csv"
    with open(out_csv, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["idx","pos","wt","mut","pred_ddg"]
        if ddg_cal is not None:
            header += ["pred_ddg_cal"]
        w.writerow(header)
        for i, (p,w0,a,_) in enumerate(muts):
            row = [i, p, w0, a, float(ddg[i])]
            if ddg_cal is not None:
                row += [float(ddg_cal[i])]
            w.writerow(row)
    print(f"ğŸ’¾ ç»“æœä¿å­˜ï¼š{out_csv}")

    # 8) Top-K
    ddg_for_rank = ddg_cal if ddg_cal is not None else ddg
    if args.top_mode == "neg":
        order = np.argsort(ddg_for_rank)
    elif args.top_mode == "pos":
        order = np.argsort(-ddg_for_rank)
    else:
        order = np.argsort(-np.abs(ddg_for_rank))
    top_idx = order[:args.topk]

    out_top = out_root / f"{args.tag}_topk.csv"
    with open(out_top, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["rank","pos","wt","mut","pred_ddg"] + (["pred_ddg_cal"] if ddg_cal is not None else []))
        for r, i in enumerate(top_idx, 1):
            p, wt_aa, mut_aa, _ = muts[i]
            row = [r, p, wt_aa, mut_aa, float(ddg[i])]
            if ddg_cal is not None:
                row += [float(ddg_cal[i])]
            w.writerow(row)
    print(f"ğŸ… Top-{args.topk} ä¿å­˜ï¼š{out_top}ï¼ˆæ¨¡å¼ï¼š{args.top_mode}ï¼‰")

    # 9) ç«å±±å›¾ï¼ˆç®€å•ç‰ˆï¼šx=Î”Î”Gï¼Œy=|Î”Î”G| çš„ rank-based è¿‘ä¼¼ï¼‰
    x = ddg_for_rank
    y = -np.log10((np.argsort(np.argsort(-np.abs(x)))+1) / (len(x)+1.0))  # ä¸€ä¸ªè¿‘ä¼¼å¯è§†åŒ–
    plt.figure(figsize=(7,5))
    plt.scatter(x, y, s=6, alpha=0.6)
    plt.axvline(0, color="gray", lw=1)
    plt.xlabel("Î”Î”G (model)")
    plt.ylabel("-log10(rank(|Î”Î”G|))")
    plt.title(f"Volcano-like plot: {args.tag}")
    fig_path = out_root / f"{args.tag}_volcano.png"
    plt.tight_layout()
    plt.savefig(fig_path, dpi=200)
    plt.close()
    print(f"ğŸ“ˆ ç«å±±å›¾ï¼š{fig_path}")

if __name__ == "__main__":
    main()

