#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
predict_ddg_overall_v2.py
--------------------------------
V2 ç‰ˆä¸€ä½“åŒ–é¢„æµ‹è„šæœ¬ï¼ˆä¸åŸè„šæœ¬åŒºåˆ«å¼€ï¼‰
- ç”Ÿæˆ WT çš„æ‰€æœ‰å•ç‚¹çªå˜
- ç”¨ ESM-1b åšå¥å­çº§ mean embedding
- è½½å…¥çº¿æ€§èƒ½é‡å¤´ï¼ŒæŒ‰ E(mut) - E(wt) å¾—åˆ° Î”Î”G
- å¯é€‰ï¼šè½½å…¥ç­‰è·å›å½’æ ¡å‡†å™¨ï¼Œè¾“å‡º pred_ddg_cal
- å†™å‡º ALL è¡¨ä¸ TopK è¡¨

ä¾èµ–ï¼š
  pip install torch fair-esm pandas numpy scikit-learn joblib biopython

ç”¨æ³•ç¤ºä¾‹ï¼š
  python scripts/predict_ddg_overall_v2.py \
    --wt input/WT.fasta \
    --positions 22-148 \
    --esm-ckpt models/checkpoints/esm1b_t33_650M_UR50S.pt \
    --model models/energy_H_rank96_full.pt \
    --scaler-h models/scaler_H.pkl \
    --scaler-l models/scaler_L.pkl \
    --calib models/calib_H_iso.pkl \
    --mut-dir Sample_Mutant \
    --out-dir output \
    --tag VHH \
    --topk 10 \
    --top-mode neg \
    --device cuda \
    --batch-size 8
"""

from __future__ import annotations
import argparse
import math
import re
import sys
from pathlib import Path
from typing import List, Tuple, Iterable, Optional

import numpy as np
import pandas as pd
import torch
import joblib

try:
    import esm
except Exception as e:
    print("[fatal] æœªæ‰¾åˆ° fair-esmï¼Œè¯·å…ˆ `pip install fair-esm`", file=sys.stderr)
    raise

# -------------------------------
# 1) æ ¡å‡†å™¨ï¼šä¸è®­ç»ƒæ—¶åŒåç±» + é²æ£’åŠ è½½
# -------------------------------

from sklearn.isotonic import IsotonicRegression

class DDGCalibrator:
    """ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„æ ¡å‡†å™¨å°è£…ï¼šiso å›å½’ + x_min/x_max è£å‰ªã€‚"""
    def __init__(self, x=None, y=None, x_min=None, x_max=None):
        self.iso = IsotonicRegression(out_of_bounds='clip')
        if x is not None and y is not None:
            self.iso.fit(np.asarray(x, float), np.asarray(y, float))
            self.x_min = float(np.min(x)) if x_min is None else float(x_min)
            self.x_max = float(np.max(x)) if x_max is None else float(x_max)
        else:
            self.x_min = float('-inf') if x_min is None else float(x_min)
            self.x_max = float('inf') if x_max is None else float(x_max)

    def predict(self, x):
        x = np.asarray(x, float)
        if np.ndim(x) == 0:
            x = x[None]
        x = np.clip(x, self.x_min, self.x_max)
        y = self.iso.predict(x)
        return np.asarray(y, float)

def load_calibrator(path: str | Path) -> DDGCalibrator:
    """å…¼å®¹ obj/dict ä¸¤ç§å­˜æ¡£ï¼›å¹¶è§£å†³ joblib ååºåˆ—åŒ–æ‰¾ä¸åˆ°ç±»åé—®é¢˜ã€‚"""
    obj = joblib.load(path)
    # å­—å…¸ç‰ˆï¼š{'iso': IsotonicRegression, 'x_min':..., 'x_max':...}
    if isinstance(obj, dict) and 'iso' in obj:
        cal = DDGCalibrator.__new__(DDGCalibrator)
        cal.iso = obj['iso']
        cal.x_min = float(obj.get('x_min', -np.inf))
        cal.x_max = float(obj.get('x_max',  np.inf))
        return cal
    # å¯¹è±¡ç‰ˆï¼šç›´æ¥è¿”å›
    if hasattr(obj, 'predict') and hasattr(obj, 'iso'):
        if not hasattr(obj, 'x_min'): obj.x_min = -np.inf
        if not hasattr(obj, 'x_max'): obj.x_max =  np.inf
        return obj
    raise ValueError(f"Unsupported calibrator object at {path}: {type(obj)}")

def apply_calibration(cal: DDGCalibrator, arr: np.ndarray) -> np.ndarray:
    y = cal.predict(arr)
    return np.asarray(y, float).reshape(-1)


# -------------------------------
# 2) å®ç”¨å‡½æ•°
# -------------------------------

AA20 = list("ACDEFGHIKLMNPQRSTVWY")

def read_single_fasta(path: str | Path) -> str:
    txt = Path(path).read_text().strip()
    # æ”¯æŒç®€æ˜“ fasta ä¸çº¯åºåˆ—
    if txt.startswith(">"):
        lines = [ln.strip() for ln in txt.splitlines() if ln and not ln.startswith(">")]
        seq = "".join(lines)
    else:
        seq = "".join([c for c in txt.splitlines() if c and not c.startswith(">")])
    seq = re.sub(r"[^A-Za-z]", "", seq).upper()
    return seq

def parse_positions(spec: str) -> List[int]:
    """'22-148' æˆ– '22,23,45' -> 1-based ä½ç½® list"""
    spec = spec.strip()
    if "-" in spec:
        a, b = spec.split("-")
        a, b = int(a), int(b)
        return list(range(a, b + 1))
    parts = [int(x) for x in re.split(r"[,\s]+", spec) if x]
    return parts

def enumerate_mutants(seq: str, positions: List[int], alts: Iterable[str]) -> Tuple[List[str], List[Tuple[int,str,str]]]:
    """è¿”å›æ‰€æœ‰å•çªå˜çš„åºåˆ—åˆ—è¡¨ã€ä»¥åŠ (pos, wt, mut) åˆ—è¡¨"""
    seq = seq.upper()
    n = len(seq)
    alts = [a.upper() for a in alts]
    muts = []
    records = []
    for pos1 in positions:
        if pos1 < 1 or pos1 > n:
            continue
        wt = seq[pos1 - 1]
        for a in alts:
            if a == wt: 
                continue
            new_seq = seq[:pos1-1] + a + seq[pos1:]
            muts.append(new_seq)
            records.append((pos1, wt, a))
    return muts, records

def load_esm_model_and_alphabet(ckpt: str | Path):
    # æ³¨æ„ï¼šlocal æ¥å£ï¼Œä¸èƒ½ä¼  regression_location
    model, alphabet = esm.pretrained.load_model_and_alphabet_local(str(ckpt))
    model.eval()
    return model, alphabet

@torch.no_grad()
def embed_mean(model, alphabet, seqs: List[str], device="cuda", batch_size=8) -> np.ndarray:
    """å¯¹æ¯æ¡åºåˆ—è®¡ç®— token å¹³å‡ï¼ˆå»æ‰ BOS/EOSï¼‰ï¼Œè¿”å› (N, D)"""
    batch_converter = alphabet.get_batch_converter()
    model = model.to(device)
    outs = []

    # åˆ†æ‰¹
    for i in range(0, len(seqs), batch_size):
        sub = seqs[i:i+batch_size]
        data = [("id", s) for s in sub]
        _, _, toks = batch_converter(data)
        toks = toks.to(device)

        # ESM-1b: per-token representation at layer 33 by default
        # ç›´æ¥ç”¨æœ€åå±‚è¡¨ç¤º
        rep = model(toks, repr_layers=[model.num_layers])["representations"][model.num_layers]  # (B, L, D)
        # å»æ‰ BOS/EOSï¼šalphabet.cls_idx / alphabet.eos_idx
        mask = (toks != alphabet.cls_idx) & (toks != alphabet.eos_idx) & (toks != alphabet.padding_idx)
        mask = mask.unsqueeze(-1)  # (B, L, 1)
        rep = rep * mask
        lengths = mask.sum(dim=1)  # (B, 1)
        lengths = lengths.clamp(min=1)
        mean = rep.sum(dim=1) / lengths  # (B, D)
        outs.append(mean.detach().cpu().numpy())

    return np.vstack(outs) if outs else np.zeros((0, model.embed_dim), dtype=np.float32)

def safe_load_scaler(pkl_path: Optional[str], dim: int) -> callable:
    """è¯»ä¸åˆ° scaler å°±è¿”å›æ’ç­‰æ˜ å°„ã€‚"""
    if not pkl_path:
        return lambda x: x
    try:
        sc = joblib.load(pkl_path)
        # å°è¯• transform
        _ = sc.transform(np.zeros((1, dim), np.float32))
        return lambda x: sc.transform(x)
    except Exception as e:
        print(f"[warn] è¯»å– scaler å¤±è´¥ï¼Œä½¿ç”¨æ’ç­‰æ˜ å°„: {pkl_path} | {e}")
        return lambda x: x

def _flatten_to_dim(arr: np.ndarray, dim: int) -> np.ndarray:
    """æŠŠå„ç§ (D,), (1,D), (D,1) ç­‰å½¢çŠ¶ï¼Œç¨³å¦¥å‹æˆ (D,)ã€‚"""
    x = np.asarray(arr, dtype=np.float32)
    if x.ndim == 1:
        if x.shape[0] != dim:
            raise RuntimeError(f"æƒé‡é•¿åº¦ä¸åŒ¹é…: {x.shape} vs {dim}")
        return x
    if x.ndim == 2:
        r, c = x.shape
        # å¸¸è§ï¼š (1, D) / (D, 1)
        if r == 1 and c == dim:
            return x.reshape(-1)
        if c == 1 and r == dim:
            return x.reshape(-1)
        # æœ‰äº›å¯¼å‡ºä¼šç»™ (D,) -> (1,D) æˆ– (D,1)ï¼Œä¸Šé¢å·²æ¶µç›–
        # é€€è·¯ï¼šå¦‚æœå…¶ä¸­ä¸€ç»´ç­‰äº dimï¼Œé€‰è¿™ä¸€ç»´
        if r == dim:
            return x[:, 0].reshape(-1) if c == 1 else x[0:dim, 0].reshape(-1)
        if c == dim:
            return x[0, :].reshape(-1) if r == 1 else x[0, 0:dim].reshape(-1)
    # å…¶ä»–ç»´åº¦ï¼Œå°½é‡ squeeze åå†åˆ¤æ–­
    xs = np.squeeze(x)
    if xs.ndim == 1 and xs.shape[0] == dim:
        return xs.astype(np.float32)
    raise RuntimeError(f"æ— æ³•å°†æƒé‡ reshape åˆ° (D,): got {x.shape}, dim={dim}")


def infer_weight_heads(state: dict, dim: int) -> Tuple[np.ndarray, float]:
    """
    ä» state é‡Œå°½å¯èƒ½æ‰¾åˆ°ä¸€ä¸ªçº¿æ€§å¤´ï¼š
      - ä¼˜å…ˆ ['wH.weight', 'wH'] / ä»¥åŠ bias é”®
      - å…¶æ¬¡ ['w.weight', 'head.weight', 'H.weight']
      - è‹¥éƒ½æ²¡æœ‰ï¼Œå¤‡é€‰ ['U.weight', 'V.weight'] ä¸­èƒ½å±•å¹³æˆ (D,) çš„ä¸€ä¸ª
    """
    if not isinstance(state, dict):
        raise RuntimeError(f"state ä¸æ˜¯ dict: {type(state)}")

    keys = list(state.keys())

    # å¸¸è§åµŒå¥—å±•å¼€
    for nest_key in ["state", "state_dict", "model_state_dict"]:
        if nest_key in state and isinstance(state[nest_key], dict):
            state = state[nest_key]
            keys = list(state.keys())
            break

    # 1) ä¼˜å…ˆ H å¤´
    wt_candidates = ["wH.weight", "wH", "weightH", "linear.weightH"]
    bias_candidates = ["wH.bias", "biasH", "linear.biasH", "bias", "b"]

    for k in wt_candidates:
        if k in state:
            w = _flatten_to_dim(np.asarray(state[k]), dim)
            # æ‰¾ bias
            b = 0.0
            for kb in bias_candidates:
                if kb in state:
                    b = float(np.asarray(state[kb]).reshape(-1)[0])
                    break
            print(f"[info] ä½¿ç”¨æƒé‡é”®: {k}, bias é”®: {kb if 'kb' in locals() else '(none)'}")
            return w, b

    # 2) é€€åŒ–åˆ°é€šç”¨å¤´
    wt_candidates2 = ["w.weight", "w", "head.weight", "H.weight", "linear.weight"]
    bias_candidates2 = ["bias", "head.bias", "H.bias", "linear.bias", "b"]

    for k in wt_candidates2:
        if k in state:
            w = _flatten_to_dim(np.asarray(state[k]), dim)
            b = 0.0
            for kb in bias_candidates2:
                if kb in state:
                    b = float(np.asarray(state[kb]).reshape(-1)[0])
                    break
            print(f"[info] ä½¿ç”¨æƒé‡é”®: {k}, bias é”®: {kb if 'kb' in locals() else '(none)'}")
            return w, b

    # 3) å†ä¸è¡Œï¼Œå°è¯• U/V é‡Œæ‰¾èƒ½å±•å¹³æˆ (D,) çš„ä¸€ä¸ª
    for k in ["U.weight", "V.weight", "U", "V"]:
        if k in state:
            try:
                w = _flatten_to_dim(np.asarray(state[k]), dim)
                print(f"[warn] æœªæ‰¾åˆ°æ˜¾å¼çº¿æ€§å¤´ï¼Œé€€è€Œç”¨ {k} ä½œä¸º wï¼ˆæ—  biasï¼‰")
                return w, 0.0
            except Exception:
                pass

    raise RuntimeError(f"æœªæ‰¾åˆ°çº¿æ€§æƒé‡å‘é‡ï¼Œstate keys(ç¤ºä¾‹)={keys[:20]} ...")


def energy_linear(z: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:
    """ E = z @ w + b """
    return z @ w + b


# -------------------------------
# 3) ä¸»æµç¨‹
# -------------------------------

def main():
    import argparse, os, math, csv, json, pickle, joblib, torch
    import numpy as np
    from pathlib import Path
    import matplotlib.pyplot as plt
    from esm import pretrained

    # ---------- utils inlined ----------
    def parse_positions(spec: str) -> list[int]:
        """æ”¯æŒ '22-148' æˆ– '22,23,25-30' """
        out = []
        for token in spec.split(","):
            token = token.strip()
            if "-" in token:
                a, b = token.split("-")
                out.extend(list(range(int(a), int(b) + 1)))
            else:
                out.append(int(token))
        return out

    def read_single_fasta(path: str) -> str:
        seq = []
        with open(path, "r") as f:
            for line in f:
                line = line.strip()
                if not line: 
                    continue
                if line.startswith(">"): 
                    continue
                seq.append(line)
        if not seq:
            raise RuntimeError(f"æ²¡æœ‰åœ¨ {path} è¯»åˆ°åºåˆ—")
        return "".join(seq).upper()

    AAs = list("ACDEFGHIKLMNPQRSTVWY")

    def build_mutants(wt: str, positions: list[int], alts: str) -> list[tuple[int, str, str]]:
        """
        è¿”å› [(pos, wt_aa, mut_aa), ...]ï¼Œpos ç”¨ 1-basedï¼ˆä¸è¾“å…¥ä¸€è‡´ï¼‰
        alts: 'ALL' æˆ– ç”± AAs ç»„æˆçš„å­—ç¬¦ä¸²ã€‚
        """
        ret = []
        for pos in positions:
            if pos < 1 or pos > len(wt):
                raise RuntimeError(f"ä½ç‚¹ {pos} è¶Šç•Œ(1..{len(wt)})")
            wt_aa = wt[pos - 1]
            cand = AAs if alts.upper() == "ALL" else [c for c in alts if c in AAs]
            for aa in cand:
                if aa != wt_aa:
                    ret.append((pos, wt_aa, aa))
        return ret

    def write_all_mut_fasta(wt: str, muts: list[tuple[int, str, str]], out_fa: str):
        """æŠŠæ‰€æœ‰å•çªå˜å†™æˆ FASTAï¼Œæ–¹ä¾¿å¤ç”¨/æ£€æŸ¥"""
        os.makedirs(os.path.dirname(out_fa), exist_ok=True)
        with open(out_fa, "w") as w:
            for i, (pos, wt_aa, mu) in enumerate(muts, 1):
                s = list(wt)
                s[pos - 1] = mu
                s = "".join(s)
                w.write(f">{i}|pos={pos}|{wt_aa}->{mu}\n")
                w.write(s + "\n")

    def load_esm1b_from_ckpt(ckpt: Path, device: str):
        model, alphabet = pretrained.load_model_and_alphabet_local(str(ckpt))
        model.eval()
        model = model.to(device)
        return model, alphabet

    @torch.no_grad()
    def esm_embed_mean(model, alphabet, seqs: list[str], device: str, batch_size: int) -> np.ndarray:
        """
        å¯¹æ¯æ¡åºåˆ—æå– ESM-1b æœ€åä¸€å±‚ token è¡¨å¾å¹¶å–å¹³å‡ï¼ˆå»æ‰ BOS/EOS/PADï¼‰ã€‚
        è¿”å› [N, D]
        """
        batch_converter = alphabet.get_batch_converter()
        layer = model.num_layers  # ESM-1b: 33
        outs = []
        for i in range(0, len(seqs), batch_size):
            batch = [("seq", s) for s in seqs[i:i + batch_size]]
            _, _, toks = batch_converter(batch)
            toks = toks.to(device)
            out = model(toks, repr_layers=[layer], return_contacts=False)
            reps = out["representations"][layer]   # [B, L, D]
            # æ©ç ï¼šé PAD/CLS/EOS
            mask = (toks != alphabet.padding_idx) & (toks != alphabet.cls_idx) & (toks != alphabet.eos_idx)
            # å¯¹æ¯ä¸ªæ ·æœ¬åš masked mean
            for b in range(reps.size(0)):
                m = mask[b]
                vec = reps[b][m].mean(dim=0)   # [D]
                outs.append(vec.cpu().numpy())
        return np.stack(outs, axis=0).astype(np.float32)

    def load_calibrator(path: str):
        """
        å…¼å®¹ä¸¤ç§ä¿å­˜æ–¹å¼ï¼š
          1) ç›´æ¥ joblib.dump(cal_obj)    -> ååºåˆ—åŒ– cal å¯¹è±¡
          2) joblib.dump({"iso": iso, "x_min":..., "x_max":...})
        è¿”å› (cal, x_min, x_max)
        """
        obj = joblib.load(path)
        if isinstance(obj, dict) and "iso" in obj:
            iso = obj["iso"]
            x_min = float(obj.get("x_min", -np.inf))
            x_max = float(obj.get("x_max",  np.inf))
            return iso, x_min, x_max
        # ç›´æ¥æ˜¯å¯¹è±¡ï¼šå°è¯•ä»å¯¹è±¡ä¸Šè¯»èŒƒå›´ï¼Œè¯»ä¸åˆ°å°±ç”¨æ— ç©·
        x_min = getattr(obj, "x_min", -np.inf)
        x_max = getattr(obj, "x_max",  np.inf)
        return obj, float(x_min), float(x_max)

    def apply_calibrator(cal, x_min, x_max, x: np.ndarray) -> np.ndarray:
        xx = np.clip(x, x_min, x_max)
        return cal.predict(xx.reshape(-1, 1)).reshape(-1)

    def plot_volcano(x: np.ndarray, out_png: str, title: str):
        os.makedirs(os.path.dirname(out_png), exist_ok=True)
        plt.figure(figsize=(5.0, 4.2), dpi=160)
        xs = np.arange(len(x))
        plt.scatter(xs, x, s=6, alpha=0.6)
        plt.axhline(0.0, color="gray", ls="--", lw=0.8)
        plt.xlabel("mutants")
        plt.ylabel("Î”Î”G (pred)")
        plt.title(title)
        plt.tight_layout()
        plt.savefig(out_png)
        plt.close()

    # ---------- args ----------
    p = argparse.ArgumentParser()
    p.add_argument("--wt", required=True, help="WT fasta")
    p.add_argument("--positions", required=True, help="å¦‚ 22-148 æˆ– 22,23,25-30")
    p.add_argument("--alts", default="ALL", help="å¤‡é€‰æ°¨åŸºé…¸é›†åˆ, e.g. 'ALL' or 'ACDE...'; é»˜è®¤ ALL")
    p.add_argument("--esm-ckpt", required=True, help="ESM-1b æƒé‡ (esm1b_t33_650M_UR50S.pt)")
    p.add_argument("--device", default="cuda")
    p.add_argument("--batch-size", type=int, default=8)
    # çº¿æ€§å¤´ + (å¯é€‰) æ ¡å‡†å™¨
    p.add_argument("--model", required=True, help="energy .pt (å«çº¿æ€§å¤´æƒé‡)")
    p.add_argument("--calib", default=None, help="å¯é€‰ï¼šæ ¡å‡†å™¨ pkl")
    # è¾“å‡º & é€‰æ‹©
    p.add_argument("--mut-dir", default="Sample_Mutant")
    p.add_argument("--out-dir", default="output")
    p.add_argument("--tag", default="VHH")
    p.add_argument("--topk", type=int, default=10)
    p.add_argument("--top-mode", choices=["neg", "pos"], default="neg",
                   help="é€‰ topkï¼šneg=Î”Î”G æœ€è´Ÿ; pos=Î”Î”G æœ€æ­£")
    args = p.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)

    # ---------- 1. è¯»å– WT & ç”Ÿæˆå…¨éƒ¨å•çªå˜ ----------
    wt = read_single_fasta(args.wt)
    pos_list = parse_positions(args.positions)
    muts = build_mutants(wt, pos_list, args.alts)
    print(f"âœ… æ€»ä½“æ¨¡å¼ | ä½ç‚¹æ•°={len(pos_list)} | å•çªå˜æ•°={len(muts)} | alts={args.alts}")

    # è®°å½•æ‰€æœ‰çªå˜åºåˆ—åˆ° FASTAï¼ˆæ–¹ä¾¿æ£€æŸ¥/å¤ç”¨ï¼‰
    all_fa = Path(args.mut_dir) / args.tag / "ALL_mutants.fasta"
    write_all_mut_fasta(wt, muts, str(all_fa))
    print(f"ğŸ§¬ æ‰€æœ‰çªå˜åºåˆ—ä¿å­˜ï¼š{all_fa}")

    # ---------- 2. ESM åµŒå…¥ ----------
    print(f"ğŸ§  ä½¿ç”¨æœ¬åœ° ESM-1b æƒé‡ï¼š{args.esm_ckpt}")
    model, alphabet = load_esm1b_from_ckpt(Path(args.esm_ckpt), args.device)
    # WT + mutants çš„å‡å€¼åµŒå…¥
    mut_seqs = []
    for pos, wt_aa, mu in muts:
        s = list(wt); s[pos-1] = mu; mut_seqs.append("".join(s))

    wt_vec = esm_embed_mean(model, alphabet, [wt], args.device, batch_size=1)[0]   # [D]
    mut_vecs = esm_embed_mean(model, alphabet, mut_seqs, args.device, batch_size=args.batch_size)  # [N,D]
    dim = wt_vec.shape[0]
    print(f"[embed] D = {dim}, wt_vec norm={float(np.linalg.norm(wt_vec)):.4f}")

    # ---------- 3. è¯»å–èƒ½é‡å¤´ ----------
    state = torch.load(args.model, map_location="cpu")
    # å¯èƒ½å¤–å±‚åµŒå¥—äº†ä¸€å±‚ dict
    for nest_key in ["state", "state_dict", "model_state_dict"]:
        if isinstance(state, dict) and nest_key in state and isinstance(state[nest_key], dict):
            state = state[nest_key]
            break

    w, b = infer_weight_heads(state, dim)   # â† ç”¨æˆ‘ç»™ä½ çš„å¢å¼ºç‰ˆ
    # ç»Ÿä¸€ä¸º numpy
    w = np.asarray(w, dtype=np.float32).reshape(-1)    # (D,)
    b = float(b)
    print(f"[head] |w|={float(np.linalg.norm(w)):.4f} , b={b:.4f}")

    # ---------- 4. è®¡ç®— Î”Î”G ----------
    # Î”G(mut) = wÂ·z_mut + bï¼› Î”G(wt) = wÂ·z_wt + bï¼›Î”Î”G = Î”G(mut)-Î”G(wt) = wÂ·(z_mut - z_wt)
    base = float(np.dot(w, wt_vec))
    ddg = (mut_vecs @ w) - base          # (N,)

    # ---------- 5. å¯é€‰æ ¡å‡† ----------
    ddg_cal = None
    if args.calib:
        try:
            cal, x_min, x_max = load_calibrator(args.calib)
            ddg_cal = apply_calibrator(cal, x_min, x_max, ddg.copy())
            print(f"[calib] ä½¿ç”¨ {args.calib} ï¼ŒèŒƒå›´ [{x_min:.4g}, {x_max:.4g}]")
        except Exception as e:
            print(f"[warn] åŠ è½½æ ¡å‡†å™¨å¤±è´¥ï¼Œè·³è¿‡ï¼š{e}")

    # ---------- 6. å¯¼å‡º CSV & TopK & ç«å±±å›¾ ----------
    rows = []
    for i, (pos, wt_aa, mu) in enumerate(muts):
        rows.append({
            "idx": i + 1,
            "pos": pos,
            "wt": wt_aa,
            "mut": mu,
            "pred_ddg": float(ddg[i]),
            "pred_ddg_cal": float(ddg_cal[i]) if ddg_cal is not None else 0.0
        })

    all_csv = Path(args.out_dir) / f"{args.tag}_ALL_ddg.csv"
    with open(all_csv, "w", newline="") as wcsv:
        w = csv.DictWriter(wcsv, fieldnames=list(rows[0].keys()))
        w.writeheader()
        for r in rows:
            w.writerow(r)
    print(f"ğŸ“„ å…¨é‡ç»“æœä¿å­˜ï¼š{all_csv}")

    # TopK
    key = "pred_ddg"
    sign = -1 if args.top_mode == "neg" else +1
    sorted_idx = sorted(range(len(rows)), key=lambda i: sign * rows[i][key])[:args.topk]
    top_rows = [{"rank": r+1, **{k: rows[i][k] for k in ["pos", "wt", "mut", key]}} for r, i in enumerate(sorted_idx)]
    top_csv = Path(args.out_dir) / f"{args.tag}_top{args.topk}_{args.top_mode}.csv"
    with open(top_csv, "w", newline="") as wcsv:
        w = csv.DictWriter(wcsv, fieldnames=list(top_rows[0].keys()))
        w.writeheader()
        for r in top_rows:
            w.writerow(r)
    print(f"ğŸ Top{args.topk}ï¼ˆæŒ‰ {args.top_mode}ï¼‰ä¿å­˜ï¼š{top_csv}")

    # ç«å±±å›¾ï¼ˆå¯è§†åŒ– Î”Î”G åˆ†å¸ƒï¼›ä¸¥æ ¼è¯´ä¸æ˜¯ logFC/âˆ’logP çš„ç«å±±ï¼Œåªä½œå¿«é€Ÿæ¦‚è§ˆï¼‰
    volcano_png = Path(args.out_dir) / f"{args.tag}_volcano.png"
    plot_volcano(ddg, str(volcano_png), f"{args.tag} Î”Î”G")
    print(f"ğŸ–¼ï¸ ç«å±±å›¾ä¿å­˜ï¼š{volcano_png}")

    print("âœ… ç»“æŸ")


if __name__ == "__main__":
    main()
